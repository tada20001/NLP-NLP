{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b625ef44",
   "metadata": {},
   "source": [
    "* 페이스북에서 만든 FastText\n",
    "* Word2vec 확장이지만, 차이점은 Word2vec은 단어를 쪼갤 수 없는 단위로 생각한다면, FastText는 하나의 단어 안에도 여러 단어들이 존재하는 것으로 간주\n",
    "* 즉, 내부단어(subword)를 고려하여 학습\n",
    "\n",
    "### 1. 내부단어(subword) 학습\n",
    "\n",
    "* 각 단어는 글자단위 n-gram의 구성으로 취급\n",
    "* n을 몇으로 결정할지에 따라 단어들이 얼마나 분리되는지 결정됨\n",
    "* 예를 들어 n을 3으로 하는 경우, apple은 app, ppl, ple로 분리하고 이들을 벡터로 만들게 됨"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4e470be2",
   "metadata": {},
   "source": [
    "n = 3 인 경우\n",
    "<ap, app, ppl, ple, le>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b769ef6e",
   "metadata": {},
   "source": [
    "* 여기에 추가적으로 하나를 더 벡터화하는데, 기존 단어에 <, 와>를 붙인 토큰임\n",
    "* 다시 말해 n = 3인 경우, FastText는 단어 apple에 대해서 다음의 6개의 토큰을 벡터화함"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2bb3b5d9",
   "metadata": {},
   "source": [
    "n = 3 인 경우\n",
    "<ap, app, ppl, ple, le>, <apple>(특별토큰)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561b472d",
   "metadata": {},
   "source": [
    "* 실제 사용할 때는 n의 최소값과 최대값으로 범위를 설정할 수 있는데, 기본값으로 각각 3과 6으로 설정되어져 있음. 다시 말해 최소값 = 3, 최대값 = 6인 경우, 단어 apple에 대해서 FastText는 아래 내부 단어들을 벡터화함"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f4532836",
   "metadata": {},
   "source": [
    "# n = 3 ~ 6인 경우\n",
    "<ap, app, ppl, ppl, le>, <app, appl, pple, ple>, <appl, pple>, ..., <apple>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b70d96",
   "metadata": {},
   "source": [
    "* 여기서 내부 단어들을 벡터화한다는 의미는 위의 단어들에 대해 Word2Vec을 수행한다는 의미\n",
    "* 내부 단어들의 벡터값을 얻었다면, 단어 apple의 벡터값은 저 위 벡터값들의 총 합으로 구성됨\n",
    "\n",
    "### 2. 모르는 단어(Out Of Vocabulary, OOV)에 대한 대응\n",
    "\n",
    "* FastText의 인공신경망을 학습한 후에는 데이터 셋의 모든 단어의 각 n-gram에 대해 워드 임베딩이 됨\n",
    "* 이렇게 되면 장점은 데이터셋만 충분하다면 위와 같은 내부 단어(subword)를 통해 모르는 단어(OOV)에 대해서도 다른 단어와의 유사도를 계산할 수 있다는 것임\n",
    "* 예를 들어, FastText에서 birthplace란 단어를 학습하지 않은 상태이지만, 다른 단어에서 birth와 place라는 내부단어가 있다면 FastText는 birthplace의 벡터를 얻을 수 있음\n",
    "\n",
    "----\n",
    "* 모르는 단어에 제대로 대체할 수 없었던 Word2Vec, GloVe와는 다른 점임\n",
    "\n",
    "### 3. 단어 집합 내 빈도수가 적었던 단어(Rare Word)에 대한 대응\n",
    "* Word2Vec의 경우 등장빈도수가 적은 단어(rare word)에 대해 임베딩 정확도가 높지 않다는 단점이 있음\n",
    "* 참고로 할 수 있는 사례가 적다보니 정확하게 임베딩이 되지 않는 것임\n",
    "* 그러나, FastText의 경우 빈도수가 적은 단어라도 그 단어의 n-gram이 다른 단어의 n-gram과 겹치게 되면 Word2Vec에 비해 비교적 높은 임베딩 벡터값을 얻음\n",
    "* FastText가 노이즈가 많은 코퍼스에서 강점을 가진 것 또는 이 때문임. 즉, 오타가 섞인 단어는 당연히 등장 빈도수가 매우 적으므로 일종의 희귀 단어가 되는데 Word2Vec에서는 오타가 섞인 단어는 임베딩이 제대로 되지 않지만 FastText는 이에 대해서도 일정 수준의 성능을 보임\n",
    "* 예를 들어 단어 apple과 오타로 p를 한 번 더 입력한 appple의 경우에는 실제로 많은 개수의 동일한 n-gram을 가짐\n",
    "\n",
    "### 4. Word2Vec vs. FastText 실습 비교\n",
    "\n",
    "#### 1) Word2Vec\n",
    "\n",
    "* 앞에서 만든 모델 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8dd24a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 모델 로드\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "model = KeyedVectors.load_word2vec_format(\"eng_w2v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "218b993c",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Key 'electrofishing' not present\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17080/3504697783.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"electrofishing\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar\u001b[1;34m(self, positive, negative, topn, clip_start, clip_end, restrict_vocab, indexer)\u001b[0m\n\u001b[0;32m    760\u001b[0m                 \u001b[0mmean\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    761\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 762\u001b[1;33m                 \u001b[0mmean\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    763\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhas_index_for\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    764\u001b[0m                     \u001b[0mall_keys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mget_vector\u001b[1;34m(self, key, norm)\u001b[0m\n\u001b[0;32m    420\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    421\u001b[0m         \"\"\"\n\u001b[1;32m--> 422\u001b[1;33m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    423\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnorm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    424\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfill_norms\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mget_index\u001b[1;34m(self, key, default)\u001b[0m\n\u001b[0;32m    394\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mdefault\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    395\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 396\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Key '{key}' not present\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    397\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    398\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"Key 'electrofishing' not present\""
     ]
    }
   ],
   "source": [
    "model.most_similar(\"electrofishing\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29f6cc8",
   "metadata": {},
   "source": [
    "* 존재하지 않는 단어를 에러 발생\n",
    "\n",
    "#### 2) FastText\n",
    "* 전처리만 이용하고 FastText 학습 코드로 변경하여 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e07a310",
   "metadata": {},
   "outputs": [],
   "source": [
    "targetXML = open('ted_en-20160408.xml', 'r', encoding='UTF8')\n",
    "target_text = etree.parse(targetXML)\n",
    "\n",
    "# xml 파일로부터 <content>와 </content> 사이의 내용만 가져온다.\n",
    "parse_text = '\\n'.join(target_text.xpath('//content/text()'))\n",
    "\n",
    "# 정규 표현식의 sub 모듈을 통해 content 중간에 등장하는 (Audio), (Laughter) 등의 배경음 부분을 제거.\n",
    "# 해당 코드는 괄호로 구성된 내용을 제거.\n",
    "content_text = re.sub(r'\\([^)]*\\)', '', parse_text)\n",
    "\n",
    "# 입력 코퍼스에 대해서 NLTK를 이용하여 문장 토큰화를 수행.\n",
    "sent_text = sent_tokenize(content_text)\n",
    "\n",
    "# 각 문장에 대해서 구두점을 제거하고, 대문자를 소문자로 변환.\n",
    "normalized_text = []\n",
    "for string in sent_text:\n",
    "     tokens = re.sub(r\"[^a-z0-9]+\", \" \", string.lower())\n",
    "     normalized_text.append(tokens)\n",
    "\n",
    "# 각 문장에 대해서 NLTK를 이용하여 단어 토큰화를 수행.\n",
    "result = [word_tokenize(sentence) for sentence in normalized_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b625ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText\n",
    "\n",
    "model = FastText(result, vector_size=100, window=5, min_count=5, workers=4, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e65488a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('electrolux', 0.8636071681976318),\n",
       " ('electrolyte', 0.8578426837921143),\n",
       " ('electroshock', 0.8490543365478516),\n",
       " ('electroencephalogram', 0.8438616394996643),\n",
       " ('electro', 0.8402094841003418),\n",
       " ('electrochemical', 0.8326069712638855),\n",
       " ('airbus', 0.8204637765884399),\n",
       " ('electrogram', 0.8197062015533447),\n",
       " ('electric', 0.81430584192276),\n",
       " ('electronic', 0.811857283115387)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"electrofishing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc42304",
   "metadata": {},
   "source": [
    "### 5. 한국어에서의 FastText\n",
    "* 한국어의 경우에도 OOV 문제 해결을 위해 FastText 적용 시도\n",
    "\n",
    "#### 1) 음절 단위\n",
    "* 음절단위의 임베딩의 경우, n=3일때 '자연어처리'라는 단어에 대해 n-gram을 만들면 다음과 같음"
   ]
  },
  {
   "cell_type": "raw",
   "id": "092f7871",
   "metadata": {},
   "source": [
    "<자연, 자연어, 연어처, 어처리, 처리>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c65d4d1",
   "metadata": {},
   "source": [
    "#### 2) 자모단위\n",
    "* 자모단위(초성, 중성, 종성 단위)로 임베딩\n",
    "* 자모단위로 하게 되면 오타나 노이즈 측면에서 더 강한 임베딩을 기대할 수 있음\n",
    "* 예를 들어,  ‘자연어처리’라는 단어는 아래와 같이 분리됨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85c0a49",
   "metadata": {},
   "source": [
    "분리된 결과 : ㅈ ㅏ _ ㅇ ㅕ ㄴ ㅇ ㅓ _ ㅊ ㅓ _ ㄹ ㅣ _"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3747fe9",
   "metadata": {},
   "source": [
    "이렇게 분리된 결과를 n=3의 n-gram을 적용하면 임베딩 결과는 다음과 같음"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e459d448",
   "metadata": {},
   "source": [
    "< ㅈ ㅏ, ㅈ ㅏ _, ㅏ _ ㅇ, ... 중략>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35071419",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1. 코퍼스 만들기 ===>> colab"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
