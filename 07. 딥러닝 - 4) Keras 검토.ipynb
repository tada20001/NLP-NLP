{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "680dc8e2",
   "metadata": {},
   "source": [
    "### 1. 전처리(Preprocessing)\n",
    "\n",
    "* Tokenizer() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14ce2a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4c8a370",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "train_text = \"The earth is an awesome place live\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1220e7ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 6, 7]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 단어 집합 생성\n",
    "tokenizer.fit_on_texts([train_text])\n",
    "\n",
    "## 정수 인코딩\n",
    "sub_text = \"The earth is an great place live\"\n",
    "sequences = tokenizer.texts_to_sequences([sub_text])[0]\n",
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b6be2c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 한국어 \n",
    "ko_text = \"케라스의 모든 기능들을 열거하는 것만으로도 한 권의 책의 분량이므로 여기서 전부 다룰 수는 없습니다. \"\n",
    "tokenizer1 = Tokenizer()\n",
    "tokenizer1.fit_on_texts([ko_text])\n",
    "ko_sequences = tokenizer1.texts_to_sequences([ko_text])[0]\n",
    "ko_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d12f8948",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 1, 'earth': 2, 'is': 3, 'an': 4, 'awesome': 5, 'place': 6, 'live': 7}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "32e07456",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'케라스의': 1,\n",
       " '모든': 2,\n",
       " '기능들을': 3,\n",
       " '열거하는': 4,\n",
       " '것만으로도': 5,\n",
       " '한': 6,\n",
       " '권의': 7,\n",
       " '책의': 8,\n",
       " '분량이므로': 9,\n",
       " '여기서': 10,\n",
       " '전부': 11,\n",
       " '다룰': 12,\n",
       " '수는': 13,\n",
       " '없습니다': 14}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer1.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8228ecb1",
   "metadata": {},
   "source": [
    "* pad_sequence() : 각 문장의 단어수가 제각각임. 모델의 입력으로 사용하려면 모든 샘플의 길이를 동일하게 맞춰야 함. 이를 자연어 처리에서 패딩(padding) 작업이라고 하는데, 보통 숫자 0을 넣어서 길이가 다른 샘플들의 길이를 맞춤. keras에서는 pad_sequence()를 사용함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ad170476",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3],\n",
       "       [4, 5, 6],\n",
       "       [0, 7, 8]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_sequences([[1, 2, 3], [3, 4, 5, 6], [7, 8]], maxlen=3, padding='pre')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8c67ad",
   "metadata": {},
   "source": [
    "* padding ='pre' : 앞에 0을 채우고 'post'의 경우 뒤에 0을 채움"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f384aaf8",
   "metadata": {},
   "source": [
    "### 2. 워드 임베딩(Word Embedding)\n",
    "\n",
    "* 텍스트 내의 단어들을 dense vector로 만드는 것을 말함\n",
    "* 원-핫 벡터에서는 대부분 0의 값을 가지고 단하나의 1의 값으로 표현, 대체로 차원이 큼\n",
    "* 워드 임베딩의 경우, 상대적으로 저차원이며 모든 원소의 값이 실수임\n",
    "* 원-핫 벡터가 주로 2000 이상의 차원이 되는 반면, 임베딩 벡터는 주로 256, 512, 1024 등의 차원을 가짐\n",
    "* 임베딩 벡터는 초기에는 랜덤 값을 가지지만, 인공신경망의 가중치가 학습되는 방법과 같은 방식으로 값이 학습되며 변경됨\n",
    "\n",
    "---\n",
    "* Embedding() : 인공신경망에서 embedding layer를 만드는 역할을 함. 정수인코딩이 된 단어들을 입력받아 임베딩 수행\n",
    "* (number of samples, input_length)인 2D  정수 텐서를 입력받고, 작업이 수행되면 (number of samples, input_length, embedding word dimension) 인 3D 텐서를 리턴함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a03dc35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.embeddings.Embedding at 0x194dbc93e80>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "### 1. 토큰화\n",
    "tokenized_text = [['Hope', 'to', 'see', 'you', 'soon'], ['Nice', 'to', 'see', 'you', 'again']]\n",
    "\n",
    "### 2. 정수인코딩\n",
    "encoded_text = [[0, 1, 2, 3, 4],[5, 1, 2, 3, 6]]\n",
    "\n",
    "### 3. 정수 인코딩 데이터가 임베딩 층의 입력이 됨\n",
    "vocab_size = 7\n",
    "embedding_dim = 2\n",
    "Embedding(vocab_size, embedding_dim, input_length=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4ba82d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = Embedding(vocab_size, embedding_dim, input_length=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c5e988da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.embeddings.Embedding at 0x194db2ed6d0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e246867c",
   "metadata": {},
   "source": [
    "* 첫번째 인자 = 단어 코퍼스 크기. 즉 단어 갯수\n",
    "* 두번째 인자 = 임베딩 벡터의 출력 차원. 임베딩 벡터의 크기\n",
    "* input_length = 입력 시퀀스의 길이"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e20cf06",
   "metadata": {},
   "source": [
    "### 3. 모델링(Modeling)\n",
    "* Sequential() : 이를 통해 model을 선언하고 add()를 통해 층을 단계적을 추가"
   ]
  },
  {
   "cell_type": "raw",
   "id": "60bd4ac7",
   "metadata": {},
   "source": [
    "model = Sequential()\n",
    "model.add(...) # 층 추가\n",
    "model.add(...) # 층 추가\n",
    "model.add(...) # 층 추가"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce9974e",
   "metadata": {},
   "source": [
    "* Embedding()을 통해 임베딩 층을 추가하는 예시"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1421d327",
   "metadata": {},
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, output_dim, input_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c025c7",
   "metadata": {},
   "source": [
    "* 전결합층(fully-connected layer)을 추가하는 예시"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9af04669",
   "metadata": {},
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(1, input_dim=3, activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca7820c",
   "metadata": {},
   "source": [
    "### 4. 컴파일과 training\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3cde5531",
   "metadata": {},
   "source": [
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38449c2e",
   "metadata": {},
   "source": [
    "* optimizer = 훈련 과정을 설정하는 옵티마이저 설정\n",
    "* loss = 손실함수 설정\n",
    "* metrics = 훈련을 모니터링하는 지표 선택\n",
    "\n",
    "문제유형에 따라 손실함수 설정 필요\n",
    "* 회귀문제 : mean_squared_error  \n",
    "* 다중 클래스 분류 : categorical_crossentropy  ==> 활성화 함수 : softmax\n",
    "* 다중 클래스 분류 : sparse_categorical_crossentropy ==> 활성화 함수 : softmax\n",
    "* 이진 분류 : binary_crossentrophy 활성화 함수 : sigmoid\n",
    "\n",
    "---\n",
    "\n",
    "* fit() : 모델 학습 기능. "
   ]
  },
  {
   "cell_type": "raw",
   "id": "6b9537d8",
   "metadata": {},
   "source": [
    "model.fit(X_train, y_train, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6ec2dfb8",
   "metadata": {},
   "source": [
    "model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=0, validation_data(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1deee00",
   "metadata": {},
   "source": [
    "* validation_data : 검증데이터 입력\n",
    "* validation_split = validation_data와 동일하게 검증 데이터를 사용하기 위한 용도로 validation_data 대신 사용할 수 있음. 검증데이터를 저장하는 것이 아니라, 훈련데이터를 일정 비율 분리하여 검증데이터로 사용"
   ]
  },
  {
   "cell_type": "raw",
   "id": "14593623",
   "metadata": {},
   "source": [
    "model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=0, validation_split=0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf2e939",
   "metadata": {},
   "source": [
    "* verbose = 학습 중 출력되는 문구 설정\n",
    "    * 0 : 아무것도 출력하지 않는다\n",
    "    * -1 : 훈련의 진행도를 보여주는 진행 막대를 보여줌\n",
    "    * -2 : 미니배치마다 손실정보 출력\n",
    "    \n",
    "### 5. 평가(Evaluation)와 예측(Prediction)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "68450e9c",
   "metadata": {},
   "source": [
    "model.evaluate(X_test, y_test, batch_size=32)\n",
    "\n",
    "model.predict(X_input, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2fdf04",
   "metadata": {},
   "source": [
    "### 6. 모델의 저장(Save)과 로드(Load)\n",
    "\n",
    "* save() : 인공 신경망 모델을 hdf5 파일에 저장"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8ec79b98",
   "metadata": {},
   "source": [
    "model.save(\"model_name.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cae7325",
   "metadata": {},
   "source": [
    "* load_model() : 저장해둔 모델을 불러옴"
   ]
  },
  {
   "cell_type": "raw",
   "id": "40597ba7",
   "metadata": {},
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "model = load_model(\"model_name.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db64393",
   "metadata": {},
   "source": [
    "https://wikidocs.net/32105"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c02039",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
