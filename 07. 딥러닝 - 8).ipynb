{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df13deb3",
   "metadata": {},
   "source": [
    "### 1. 기존 N-gram 언어 모델의 한계\n",
    "* n-gram 언어모델은 언어 모델링에 바로 앞 n-1개 의 단어를 참고함\n",
    "* 4-gram 언어 모델이라고 할때, 바로 앞 3개의 단어만 참고하며 더앞의 단어는 무시함\n",
    "\n",
    "$$P(w\\text{|boy is spreading}) = \\frac{\\text{count(boy is spreading}\\ w)}{\\text{count(boy is spreading)}}$$\n",
    "\n",
    "* 그러나 이러한 언어모델은 충분한 데이터를 관측하지 못하면 언어를 정확히 모델링하지 못하는 희소문제(sparsity problem) 발생\n",
    "\n",
    "\n",
    "### 2. 단어의 의미적 유사성\n",
    "* n-gram 언어모델은 단어의 유사도를 알 수 없으므로 단어의 의미적 유사성을 예측에 고려할 수 없음\n",
    "* 이러한 단어의 유사성을 반영한 언어모델이 NNLM임\n",
    "\n",
    "\n",
    "### 3. 피드포워드 신경망 언어 모델(NNLM)\n",
    "* NNLM은 n-gram  언어모델과 마찬가지로 다음 단어를 예측할 때 앞의 모든 단어를 참고하지 않고 정해진 개수의 단어만 참고함\n",
    "* 이 개수를 n이라고 하고 n을 4라고 했을때 앞의 4개 단어까지만 참고하고 그 앞의 단어는 무시함\n",
    "* 이 범위를 윈도우(window)라고 함\n",
    "\n",
    "---\n",
    "* NNLM의 구조 : Input Layer ==> Projection Layer ==> Hidden Layer ==> Output Layer\n",
    "\n",
    "* Input Layer : 4개 단어를 집어 넣을 경우 4-window 활용, 인코딩하여 출력값의 예측치 오차를 구하기 위한 레이블로 활용\n",
    "* Porjection Layer : 일반 hidden layer와 달리 가중치 행렬과의 곱은 이루어지지만 활성화 함수가 존재하지 않음\n",
    "    * input의 원핫벡터와 가중치 행렬 곱이 이루어짐\n",
    "    * 원핫벡터와 곱하기 때문에 실제 가중치 행렬의 행을 그대로 읽어오는 것과 동일함. 이에 이 작업을 룩업(lookup) 테이블이라고 함\n",
    "    ---\n",
    "    * 룩업 테이블 작업 이후로는 더 작은 차원의 벡터로 다시 맵핑됨\n",
    "    * 이 값은 초기에는 랜덤한 값을 가지지만 학습과정에서 계속 변경되는데 이 단어 벡터를 임베딩 벡터(embedding vector)라고 함\n",
    "    * Proejction Layer에서 모든 임베딩 벡터 값을 연결시킴(concatenate). 이 연결된 연산은 벡터들을 이어 붙이는 것으로 5차원 벡터 4개를 연결한다면 20차원 벡터를 얻게 됨\n",
    "    \n",
    "    투사층 : x - input의 원핫벡터, t - 문장에서 t번째 단어, n - 윈도우 크기, ; - 연결\n",
    "    \n",
    "    $$p^{layer} = (lookup(x_{t-n}); ...; lookup(x_{t-2}); lookup(x_{t-1})) = (e_{t-n}; ...; e_{t-2}; e_{t-1})$$\n",
    "    \n",
    "    * 일반적인 hidden layer가 활성화 함수를 사용하는 비선형층인 것과 달리, 투사층은 활성화 함수가 존재하지 않는 선형층(linear layer)이라는 점이 다름\n",
    "    * 이는 다시 은닉층을 사용하는 일반적인 피드 포워드 신경망과 동일함\n",
    "    \n",
    "* hidden layer에서는 투사층의 결과가 h의 크기를 가지는 은닉층을 지나게 됨(이후 동일)\n",
    "    * 일반적으로 NNPLM에서는 가중치가 곱해진후 편향이 더해져 활성화 함수 입력이 됨\n",
    "    \n",
    "    \n",
    "* NNLM의 핵심은 충분한 양의 훈련 코퍼스를 학습하면 결과적으로 수많은 문장에서 유사한 목적으로 사용되는 단어들은 유사한 임베딩 벡터값을 얻게 됨. 이렇게 되면 다음 단어를 예측하는 과정에서 훈련 코퍼스에 없던 새로운 단어 시퀀스라 하더라도 다음 단어를 선택할 수 있음\n",
    "\n",
    "* 단어간 유사도를 구할 수 있는 임베딩 벡터의 아이디어는 Word2Vec, FastText, GloVe 등으로 발전되어 자연어 모델 처리에 필수적으로 사용되고 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ead6f08",
   "metadata": {},
   "source": [
    "### 4. NNLM의 이점과 한계\n",
    "\n",
    "* 개선점 : 임베딩 벡터를 사용하므로 단어의 유사도 계산이 가능하고 이를 통해 희소성 문제 해결 가능\n",
    "* 한계 : n-gram 언어 모델과 마찬가지로 다음 단어 예측을 위해 이전 단어를 참고하는 것이 아니라, 정해진 n개의 단어만을 참고할 수 있음.... 이를 극복하기 위해 RNN(Recurrent Neural Network)를 사용한 RNNLM이 만들어짐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d95d9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
