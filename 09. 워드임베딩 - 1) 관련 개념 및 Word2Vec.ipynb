{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4cc983b",
   "metadata": {},
   "source": [
    "### 1. 워드 임베딩 관련 개념\n",
    "\n",
    "* 단어를 벡터로 표현하는 방법\n",
    "* 희소표현, 밀집표현, 워드임베딩 3가지 개념이 있음\n",
    "\n",
    "#### 1) 희소표현(Sparse Representation) 이슈\n",
    "\n",
    "* 웟-핫 인코딩에서 발생하는 문제\n",
    "* 공간적 낭비 문제\n",
    "* 단어의 의미를 표현하지 못함\n",
    "\n",
    "#### 2) 밀집표현(Dense Representation)\n",
    "* 벡터 차원을 단어 집합의 크기로 상정하지 않음\n",
    "* 사용자가 설정한 값으로 모든 단어의 벡터 표현의 차원을 맞춤\n",
    "* 0과 1만이 아니라 실수값도 포함\n",
    "\n",
    "#### 3) 워드 임베딩(Word Embedding)\n",
    "* 단어를 밀집벡터의 형태로 표현하는 방법을 워드 임베딩이라고 함\n",
    "* 방법론으로는 LSA, Word2Vec, FastText, Glove 등이 있음\n",
    "* Keras Embedding() 에서는 단어를 랜덤한 값을 가지는 밀집 벡터로 변환한 뒤에, 인공신경망의 가중치를 학습하는 것과 같은 방식 사용\n",
    "* 학습데이터로부터 학습한다는 점이 특징"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6439367",
   "metadata": {},
   "source": [
    "### 2. Word2Vec\n",
    "\n",
    "* 원-핫벡터의 단점은 단어 벡터간 유사도를 계산할 수 없다는 점임\n",
    "* 단어 벡터간 유의미한 유사도를 반영할 수 있도록 단어의 의미를 수치화할 수 있는 방법 중 하나\n",
    "\n",
    "#### 1) 다차원 분산표현(Distributed Representation)\n",
    "* 희소표현 대안으로 단어의 의미를 다차원 공간에 벡터화하는 방법\n",
    "* 이러한 분산표현을 이용하여 단어간 의미적 유사성을 벡터화하는 작업이 워드 임베딩임\n",
    "* 기본적으로 분포가설(distributed hypothesis)이라는 가정 하에 만들어진 표현방법\n",
    "    * 가정 : **비숫한 문맥에서 등장하는 단어들은 비슷한 의미를 가진다**\n",
    "* 다차원 분산표현은 분포가설을 이용하여 텍스트를 학습하고 여러 차원에 분산하여 표현\n",
    "* 전체 단어집합을 고려할 필요없이 사용자가 설정한 차원의 수를 가지는 벡터가 되며, 각 차원의 값은 실수값을 가짐\n",
    "* 즉, 희소표현이 고차원에 각 차원이 분리된 표현방법이라면, 분산표현은 저차원에 **단어의 의미를 여러 차원에다가 분산**하여 표현\n",
    "* 이러한 방법을 사용하면 **단어 벡터간 유의미한 유사도**를 계산할 수 있음\n",
    "* 이를 위한 대표적인 학습방법이 Word2Vec임\n",
    "\n",
    "#### 2) CBOW(Continuous Bag of Words)\n",
    "* Word2Vec 학습방식에는 CBOW와 Skip-Gram이 있음\n",
    "* CBOW : 주변에 있는 단어들(context word)을 입력하여 중심에 있는 단어(center word)들을 예측하는 방법\n",
    "* Skip-Gram : 중심에 있는 단어들을 입력하여 주변 단어를 예측하는 방법\n",
    "\n",
    "---\n",
    "* 중심단어(center word)를 예측하기 위해 앞, 뒤로 몇개의 단어를 볼지 결정해야 하는데 이 범위를 윈도우(window)라고 함\n",
    "* 윈도우 크기가 정해지면 윈도우를 옆으로 움직여서 주변단어와 중심단어의 선택을 변경해가며 학습을 위한 데이터셋을 만드는데 이 방법을 슬라이딩 윈도우(Sliding window)라고 함\n",
    "\n",
    "* Word2Vec에서 입력은 모두 원-핫 벡터가 되어야 함\n",
    "* 즉, 입력층의 입력으로 앞뒤로 상요자가 정한 윈도우 크기 범위안에 있는 주변 단어들의 원-핫 벡터가 들어가고, 출력층에서 예측하고자 하는 중심단어의 원-핫 벡터가 레이블로서 필요\n",
    "---\n",
    "* Word2Vec은 은닉층이 다수 있는 딥러닝 모델이 아니라 은닉층이 1개인 얕은 신경망(shallow neural network)이라는 점임\n",
    "* 또한, 은닉층도 일반적인 은닉층과 달리 활성화 함수가 존재하지 않으며, 룩업 데이터블이라는 연산을 담당하는 projection layer(투사층)만 있음\n",
    "\n",
    "* 이 투사층을 전후로 입력층의 가중치 $W$와 출력층의 가중치$W'$를 갖게 되는데 전자는 $V X M$, 후자는 $M X V$임. 그러나 둘의 관계는 전치가 아니고 전혀 다른 값의 행렬임\n",
    "\n",
    "* CBOW의 목적은 W와 W'를 잘 훈련시키는 것인데, 그 이유가 lookup(전 입력단계)해온 W의 각 행벡터가 Word2Vec 학습 후에는 각 단어의 M차원의 임베딩 벡터로 간주되기 때문임"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b0e563",
   "metadata": {},
   "source": [
    "#### 3) Skip-gram \n",
    "* 중심단어에서 대해 주변단어를 예측하는 것이므로 투사층에서 벡터들의 평균을 구하는 과정은 없음\n",
    "* 여러 논문의 분석결과, 성능비교시 전반적으로 Skip-gram이 CBOW보다 성능이 좋다고 알려짐\n",
    "\n",
    "#### 4) NNLM과 Word2Vec 차이\n",
    "* 예측대상이 다름 : NNLM은 다음 단어를 예측하나, Word2Vec은 중심단어(또는 주변단어)를 예측\n",
    "* 구조도 다름 : Word2Vec은 활성화함수가 있는 은닉층이 없음. 이에 투사층 다음에 바로 출력층이 연결되는 구조임. 따라서 속도도 빠름\n",
    "* "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
